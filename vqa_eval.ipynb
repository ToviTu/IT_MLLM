{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.model import EvalModel\n",
    "import os\n",
    "from scripts.datasets import VQA_dataset\n",
    "from scripts.datasets import SQUAD_dataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = (\n",
    "    \"/mnt/d/models/\"\n",
    "    if os.environ.get(\"CHECKPOINT_DIR\") == None\n",
    "    else os.environ[\"CHECKPOINT_DIR\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Checkpoint from /mnt/d/models/\n"
     ]
    }
   ],
   "source": [
    "model_args = {\n",
    "    \"vision_encoder_path\": \"ViT-L-14\",\n",
    "    \"vision_encoder_pretrained\": \"openai\",\n",
    "    \"lm_path\": \"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "    \"lm_tokenizer_path\": \"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "    \"checkpoint_path\": f\"{CHECKPOINT_DIR}/OpenFlamingo-3B-vitl-mpt1b/checkpoint.pt\",\n",
    "    \"cross_attn_every_n_layers\": 1,\n",
    "    \"precision\": \"bf16\",\n",
    "    \"device\": 0,\n",
    "}\n",
    "\n",
    "print(f\"Loading Checkpoint from {CHECKPOINT_DIR}\")\n",
    "model = EvalModel(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SQUAD_dataset()\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    final = {'text': []}\n",
    "    for d in batch:\n",
    "        contexts = d['context']\n",
    "        questions = d['question']\n",
    "        answers = d['answers']['text'][0]\n",
    "        temp = data.qa_prompt(contexts, questions, answers)\n",
    "        final['text'].append(temp)\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_d = DataLoader(data.train_dataset, batch_size=4, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1046992944 || all params: 2559117360 || trainable%: 40.91\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 22216704 || all params: 2581334064 || trainable%: 0.86\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r = 32,\n",
    "    lora_alpha = 32,\n",
    "    target_modules = ['to_q', 'to_kv', 'to_out', 'ff.1', 'ff.3'],\n",
    "    lora_dropout=0.1,\n",
    "    bias='none',\n",
    ")\n",
    "lora_model = get_peft_model(model.model, config)\n",
    "print_trainable_parameters(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21900 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/vqa_eval.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/vqa_eval.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output\u001b[39m.\u001b[39mlogits\u001b[39m.\u001b[39mreshape(\u001b[39m4\u001b[39m, \u001b[39m50280\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), input_ids[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/vqa_eval.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/vqa_eval.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/vqa_eval.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/vqa_eval.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m loss_vals\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/flamingo/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/flamingo/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(lora_model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "loss_vals = []\n",
    "for epoch in range(5):\n",
    "    counter = 0\n",
    "    for batch in tqdm.tqdm(train_d):\n",
    "\n",
    "        if counter > 2000:\n",
    "            continue\n",
    "        counter += 1\n",
    "\n",
    "        token = model.tokenizer(batch['text'])\n",
    "        image = data.palceholder_image\n",
    "        image_token = data.image_preprocess_batch(model.image_processor, [image])\n",
    "        image_tokens = torch.cat([image_token] * len(batch['text']), dim=0)\n",
    "        input_ids, attention_mask = model._prepare_text(batch['text'])\n",
    "\n",
    "        output = lora_model(\n",
    "                    image_tokens.to(0, dtype=torch.bfloat16),\n",
    "                    input_ids[:, :-1],\n",
    "                    attention_mask[:, :-1]\n",
    "                )\n",
    "        loss = criterion(output.logits.reshape(4, 50280, -1), input_ids[:, :-1])\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_vals.append(loss.cpu().detach().cpu().item())\n",
    "        torch.cuda.empty_cache()\n",
    "    print(f\"loss: {sum(loss_vals)/len(loss_vals)}\")\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246541e5c8504f168494141f5ee73815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/174M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ToviTu/fine-tuned-nl-flamingo/commit/3a47056055b63209ee8cce8620c149776c90bf46', commit_message='Upload model', commit_description='', oid='3a47056055b63209ee8cce8620c149776c90bf46', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.push_to_hub(\"ToviTu/fine-tuned-nl-flamingo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d0437395fc4db9a59c950f68c9974e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = lora_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Flamingo' object has no attribute 'push_to_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/vqa_eval.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/vqa_eval.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m merged\u001b[39m.\u001b[39;49mpush_to_hub(\u001b[39m\"\u001b[39m\u001b[39mToviTu/fine-tuned-nl-flamingo\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/flamingo/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Flamingo' object has no attribute 'push_to_hub'"
     ]
    }
   ],
   "source": [
    "merged.push_to_hub(\"ToviTu/fine-tuned-nl-flamingo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/mnt/d/models/OpenFlamingo-3B-vitl-mpt1b-ft-squad/ does not appear to have a file named config.json. Checkout 'https://huggingface.co//mnt/d/models/OpenFlamingo-3B-vitl-mpt1b-ft-squad//None' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/vqa_eval.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/vqa_eval.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/vqa_eval.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m m \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39m/mnt/d/models/OpenFlamingo-3B-vitl-mpt1b-ft-squad/\u001b[39;49m\u001b[39m\"\u001b[39;49m, local_files_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/flamingo/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:526\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mquantization_config\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    524\u001b[0m     _ \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mquantization_config\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 526\u001b[0m config, kwargs \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    527\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    528\u001b[0m     return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    529\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    530\u001b[0m     code_revision\u001b[39m=\u001b[39;49mcode_revision,\n\u001b[1;32m    531\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    532\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    533\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    534\u001b[0m )\n\u001b[1;32m    536\u001b[0m \u001b[39m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[39mif\u001b[39;00m kwargs_orig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/flamingo/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1048\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1045\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   1046\u001b[0m code_revision \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mcode_revision\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1048\u001b[0m config_dict, unused_kwargs \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1049\u001b[0m has_remote_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1050\u001b[0m has_local_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/miniconda3/envs/flamingo/lib/python3.10/site-packages/transformers/configuration_utils.py:622\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    620\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    621\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 622\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    623\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    624\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/flamingo/lib/python3.10/site-packages/transformers/configuration_utils.py:677\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    675\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    676\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    678\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    679\u001b[0m         configuration_file,\n\u001b[1;32m    680\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    681\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    682\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    683\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    684\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    685\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    686\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    687\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    688\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    689\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    690\u001b[0m     )\n\u001b[1;32m    691\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    692\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    693\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    694\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/flamingo/lib/python3.10/site-packages/transformers/utils/hub.py:401\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(resolved_file):\n\u001b[1;32m    400\u001b[0m     \u001b[39mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[0;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    402\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m does not appear to have a file named \u001b[39m\u001b[39m{\u001b[39;00mfull_filename\u001b[39m}\u001b[39;00m\u001b[39m. Checkout \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for available files.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    406\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: /mnt/d/models/OpenFlamingo-3B-vitl-mpt1b-ft-squad/ does not appear to have a file named config.json. Checkout 'https://huggingface.co//mnt/d/models/OpenFlamingo-3B-vitl-mpt1b-ft-squad//None' for available files."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "m = AutoModelForCausalLM.from_pretrained(\"/mnt/d/models/OpenFlamingo-3B-vitl-mpt1b-ft-squad/\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removed shared tensor {'old_decoder_blocks.20.ln_2.weight', 'gated_cross_attn_layers.18.attn.norm.bias', 'gated_cross_attn_layers.23.attn.to_q.weight', 'old_decoder_blocks.23.attn.Wqkv.weight', 'gated_cross_attn_layers.4.attn.norm.weight', 'gated_cross_attn_layers.15.attn.to_out.weight', 'gated_cross_attn_layers.3.attn.to_out.weight', 'old_decoder_blocks.6.mlp.mlp_up.weight', 'old_decoder_blocks.18.attn.k_ln.weight', 'gated_cross_attn_layers.13.attn.norm.bias', 'old_decoder_blocks.8.mlp.mlp_down.weight', 'gated_cross_attn_layers.11.ff.0.weight', 'gated_cross_attn_layers.12.ff.0.bias', 'gated_cross_attn_layers.0.attn_gate', 'gated_cross_attn_layers.17.attn_gate', 'gated_cross_attn_layers.9.attn_gate', 'old_decoder_blocks.14.attn.out_proj.weight', 'gated_cross_attn_layers.6.attn.to_kv.weight', 'gated_cross_attn_layers.11.attn.to_out.weight', 'gated_cross_attn_layers.8.attn.norm.weight', 'gated_cross_attn_layers.12.attn.norm.bias', 'gated_cross_attn_layers.9.ff_gate', 'gated_cross_attn_layers.10.attn.to_kv.weight', 'gated_cross_attn_layers.14.attn.norm.bias', 'old_decoder_blocks.17.attn.Wqkv.weight', 'old_decoder_blocks.18.ln_1.weight', 'gated_cross_attn_layers.23.attn.norm.weight', 'gated_cross_attn_layers.2.ff.0.bias', 'gated_cross_attn_layers.1.attn.to_q.weight', 'old_decoder_blocks.4.mlp.mlp_up.weight', 'gated_cross_attn_layers.20.attn.norm.weight', 'gated_cross_attn_layers.17.attn.norm.bias', 'gated_cross_attn_layers.4.ff.3.weight', 'gated_cross_attn_layers.8.attn_gate', 'gated_cross_attn_layers.19.ff.0.weight', 'gated_cross_attn_layers.11.ff.1.weight', 'gated_cross_attn_layers.14.attn.norm.weight', 'gated_cross_attn_layers.3.attn_gate', 'old_decoder_blocks.15.ln_2.weight', 'gated_cross_attn_layers.10.attn.norm.bias', 'gated_cross_attn_layers.19.ff.0.bias', 'old_decoder_blocks.22.ln_2.weight', 'gated_cross_attn_layers.11.ff_gate', 'gated_cross_attn_layers.23.ff_gate', 'gated_cross_attn_layers.11.ff.3.weight', 'old_decoder_blocks.10.attn.q_ln.weight', 'gated_cross_attn_layers.22.ff.3.weight', 'old_decoder_blocks.2.attn.k_ln.weight', 'gated_cross_attn_layers.17.attn.to_out.weight', 'old_decoder_blocks.16.attn.out_proj.weight', 'gated_cross_attn_layers.18.attn.to_kv.weight', 'gated_cross_attn_layers.0.ff.0.weight', 'old_decoder_blocks.14.attn.q_ln.weight', 'old_decoder_blocks.20.mlp.mlp_down.weight', 'gated_cross_attn_layers.7.attn.norm.bias', 'old_decoder_blocks.11.attn.out_proj.weight', 'gated_cross_attn_layers.2.ff.0.weight', 'gated_cross_attn_layers.18.ff.3.weight', 'gated_cross_attn_layers.7.attn.to_out.weight', 'old_decoder_blocks.16.ln_1.weight', 'old_decoder_blocks.3.attn.out_proj.weight', 'gated_cross_attn_layers.1.ff.3.weight', 'gated_cross_attn_layers.21.ff_gate', 'old_decoder_blocks.20.attn.k_ln.weight', 'gated_cross_attn_layers.13.attn.norm.weight', 'gated_cross_attn_layers.21.attn.norm.weight', 'gated_cross_attn_layers.6.ff.1.weight', 'gated_cross_attn_layers.13.attn_gate', 'gated_cross_attn_layers.13.attn.to_kv.weight', 'old_decoder_blocks.19.mlp.mlp_down.weight', 'old_decoder_blocks.12.ln_2.weight', 'gated_cross_attn_layers.15.ff.3.weight', 'gated_cross_attn_layers.7.ff.0.weight', 'gated_cross_attn_layers.3.ff.3.weight', 'old_decoder_blocks.10.ln_1.weight', 'old_decoder_blocks.8.attn.k_ln.weight', 'old_decoder_blocks.17.mlp.mlp_down.weight', 'old_decoder_blocks.12.attn.Wqkv.weight', 'gated_cross_attn_layers.13.ff.0.bias', 'gated_cross_attn_layers.19.attn.to_kv.weight', 'gated_cross_attn_layers.6.attn_gate', 'old_decoder_blocks.12.mlp.mlp_down.weight', 'gated_cross_attn_layers.16.attn.to_out.weight', 'gated_cross_attn_layers.0.ff_gate', 'old_decoder_blocks.6.ln_2.weight', 'old_decoder_blocks.13.attn.Wqkv.weight', 'old_decoder_blocks.21.attn.q_ln.weight', 'gated_cross_attn_layers.23.ff.3.weight', 'gated_cross_attn_layers.9.attn.norm.bias', 'old_decoder_blocks.5.attn.Wqkv.weight', 'gated_cross_attn_layers.16.attn.norm.weight', 'old_decoder_blocks.5.attn.k_ln.weight', 'gated_cross_attn_layers.8.ff.0.bias', 'old_decoder_blocks.9.attn.out_proj.weight', 'old_decoder_blocks.22.attn.k_ln.weight', 'gated_cross_attn_layers.10.attn.norm.weight', 'old_decoder_blocks.19.ln_1.weight', 'old_decoder_blocks.22.mlp.mlp_up.weight', 'gated_cross_attn_layers.20.ff.1.weight', 'gated_cross_attn_layers.9.ff.1.weight', 'gated_cross_attn_layers.15.attn.to_q.weight', 'gated_cross_attn_layers.2.ff.1.weight', 'old_decoder_blocks.23.mlp.mlp_down.weight', 'old_decoder_blocks.9.attn.Wqkv.weight', 'gated_cross_attn_layers.21.ff.0.weight', 'old_decoder_blocks.23.mlp.mlp_up.weight', 'gated_cross_attn_layers.17.ff_gate', 'gated_cross_attn_layers.18.attn.to_q.weight', 'old_decoder_blocks.4.attn.out_proj.weight', 'old_decoder_blocks.18.mlp.mlp_up.weight', 'gated_cross_attn_layers.12.ff.3.weight', 'gated_cross_attn_layers.5.ff.0.weight', 'old_decoder_blocks.10.attn.Wqkv.weight', 'gated_cross_attn_layers.6.ff.0.weight', 'old_decoder_blocks.11.mlp.mlp_down.weight', 'gated_cross_attn_layers.16.attn.to_kv.weight', 'gated_cross_attn_layers.8.ff.3.weight', 'old_decoder_blocks.17.mlp.mlp_up.weight', 'gated_cross_attn_layers.7.ff_gate', 'gated_cross_attn_layers.11.attn.norm.bias', 'gated_cross_attn_layers.3.attn.norm.weight', 'old_decoder_blocks.23.ln_1.weight', 'gated_cross_attn_layers.13.ff_gate', 'old_decoder_blocks.5.ln_1.weight', 'gated_cross_attn_layers.17.attn.norm.weight', 'gated_cross_attn_layers.8.ff_gate', 'gated_cross_attn_layers.16.attn.norm.bias', 'gated_cross_attn_layers.1.attn.norm.bias', 'old_decoder_blocks.2.attn.Wqkv.weight', 'old_decoder_blocks.0.attn.q_ln.weight', 'gated_cross_attn_layers.22.attn_gate', 'gated_cross_attn_layers.20.attn.to_q.weight', 'old_decoder_blocks.5.attn.q_ln.weight', 'gated_cross_attn_layers.6.attn.norm.bias', 'gated_cross_attn_layers.0.ff.1.weight', 'gated_cross_attn_layers.7.ff.0.bias', 'old_decoder_blocks.19.attn.q_ln.weight', 'old_decoder_blocks.18.attn.q_ln.weight', 'gated_cross_attn_layers.20.ff.3.weight', 'old_decoder_blocks.23.attn.q_ln.weight', 'gated_cross_attn_layers.4.attn_gate', 'gated_cross_attn_layers.23.attn.to_out.weight', 'gated_cross_attn_layers.6.attn.to_out.weight', 'old_decoder_blocks.18.mlp.mlp_down.weight', 'gated_cross_attn_layers.17.ff.3.weight', 'old_decoder_blocks.0.attn.Wqkv.weight', 'old_decoder_blocks.21.attn.k_ln.weight', 'gated_cross_attn_layers.7.ff.1.weight', 'gated_cross_attn_layers.5.attn.to_out.weight', 'old_decoder_blocks.17.attn.k_ln.weight', 'gated_cross_attn_layers.18.ff.0.weight', 'gated_cross_attn_layers.12.attn.norm.weight', 'gated_cross_attn_layers.1.ff.1.weight', 'old_decoder_blocks.1.mlp.mlp_up.weight', 'gated_cross_attn_layers.22.ff.0.bias', 'gated_cross_attn_layers.18.attn_gate', 'old_decoder_blocks.8.mlp.mlp_up.weight', 'old_decoder_blocks.14.mlp.mlp_up.weight', 'gated_cross_attn_layers.1.attn.to_kv.weight', 'gated_cross_attn_layers.9.ff.0.weight', 'old_decoder_blocks.5.mlp.mlp_down.weight', 'old_decoder_blocks.12.attn.q_ln.weight', 'old_decoder_blocks.20.ln_1.weight', 'gated_cross_attn_layers.4.attn.to_kv.weight', 'gated_cross_attn_layers.10.ff_gate', 'old_decoder_blocks.13.attn.q_ln.weight', 'old_decoder_blocks.17.ln_2.weight', 'gated_cross_attn_layers.1.attn.to_out.weight', 'gated_cross_attn_layers.23.attn_gate', 'gated_cross_attn_layers.0.attn.to_kv.weight', 'old_decoder_blocks.16.attn.q_ln.weight', 'gated_cross_attn_layers.10.attn.to_out.weight', 'gated_cross_attn_layers.12.ff_gate', 'gated_cross_attn_layers.3.attn.to_kv.weight', 'gated_cross_attn_layers.16.ff_gate', 'gated_cross_attn_layers.20.attn_gate', 'old_decoder_blocks.1.ln_1.weight', 'gated_cross_attn_layers.20.attn.to_kv.weight', 'gated_cross_attn_layers.20.ff.0.weight', 'old_decoder_blocks.19.attn.k_ln.weight', 'old_decoder_blocks.19.mlp.mlp_up.weight', 'gated_cross_attn_layers.23.ff.1.weight', 'gated_cross_attn_layers.2.attn.norm.weight', 'gated_cross_attn_layers.16.ff.0.weight', 'old_decoder_blocks.16.mlp.mlp_up.weight', 'gated_cross_attn_layers.21.ff.3.weight', 'old_decoder_blocks.22.attn.Wqkv.weight', 'gated_cross_attn_layers.20.ff_gate', 'old_decoder_blocks.1.attn.Wqkv.weight', 'gated_cross_attn_layers.6.attn.to_q.weight', 'gated_cross_attn_layers.3.ff_gate', 'old_decoder_blocks.4.ln_1.weight', 'gated_cross_attn_layers.2.ff_gate', 'gated_cross_attn_layers.11.attn.to_kv.weight', 'gated_cross_attn_layers.11.attn.norm.weight', 'gated_cross_attn_layers.15.attn.norm.weight', 'old_decoder_blocks.21.mlp.mlp_up.weight', 'gated_cross_attn_layers.12.ff.0.weight', 'gated_cross_attn_layers.15.ff.0.weight', 'gated_cross_attn_layers.8.attn.to_out.weight', 'old_decoder_blocks.7.ln_2.weight', 'gated_cross_attn_layers.17.ff.0.weight', 'old_decoder_blocks.5.attn.out_proj.weight', 'gated_cross_attn_layers.2.ff.3.weight', 'gated_cross_attn_layers.11.attn_gate', 'gated_cross_attn_layers.14.attn.to_kv.weight', 'gated_cross_attn_layers.19.attn.to_out.weight', 'old_decoder_blocks.0.mlp.mlp_up.weight', 'gated_cross_attn_layers.1.attn.norm.weight', 'gated_cross_attn_layers.20.attn.norm.bias', 'gated_cross_attn_layers.13.ff.3.weight', 'gated_cross_attn_layers.14.attn_gate', 'old_decoder_blocks.0.ln_1.weight', 'gated_cross_attn_layers.16.ff.3.weight', 'gated_cross_attn_layers.14.ff_gate', 'old_decoder_blocks.11.attn.k_ln.weight', 'gated_cross_attn_layers.1.ff.0.bias', 'gated_cross_attn_layers.4.ff_gate', 'gated_cross_attn_layers.17.attn.to_q.weight', 'old_decoder_blocks.0.attn.out_proj.weight', 'gated_cross_attn_layers.22.attn.to_kv.weight', 'gated_cross_attn_layers.4.attn.to_out.weight', 'gated_cross_attn_layers.1.attn_gate', 'gated_cross_attn_layers.18.ff.0.bias', 'old_decoder_blocks.20.attn.out_proj.weight', 'gated_cross_attn_layers.21.attn.norm.bias', 'gated_cross_attn_layers.7.attn.to_kv.weight', 'gated_cross_attn_layers.8.attn.to_q.weight', 'old_decoder_blocks.22.mlp.mlp_down.weight', 'old_decoder_blocks.15.mlp.mlp_down.weight', 'gated_cross_attn_layers.13.ff.1.weight', 'old_decoder_blocks.21.attn.out_proj.weight', 'gated_cross_attn_layers.2.attn.to_out.weight', 'old_decoder_blocks.8.ln_1.weight', 'old_decoder_blocks.10.mlp.mlp_up.weight', 'gated_cross_attn_layers.13.attn.to_out.weight', 'gated_cross_attn_layers.16.ff.1.weight', 'gated_cross_attn_layers.14.ff.0.bias', 'gated_cross_attn_layers.15.ff.1.weight', 'gated_cross_attn_layers.8.attn.to_kv.weight', 'gated_cross_attn_layers.20.attn.to_out.weight', 'gated_cross_attn_layers.12.attn.to_out.weight', 'gated_cross_attn_layers.19.ff.3.weight', 'old_decoder_blocks.1.attn.k_ln.weight', 'gated_cross_attn_layers.9.attn.to_q.weight', 'gated_cross_attn_layers.21.attn.to_out.weight', 'gated_cross_attn_layers.22.ff_gate', 'gated_cross_attn_layers.7.attn_gate', 'gated_cross_attn_layers.13.attn.to_q.weight', 'gated_cross_attn_layers.19.attn_gate', 'old_decoder_blocks.15.attn.q_ln.weight', 'gated_cross_attn_layers.4.attn.norm.bias', 'gated_cross_attn_layers.5.ff.0.bias', 'old_decoder_blocks.3.attn.Wqkv.weight', 'gated_cross_attn_layers.0.attn.norm.weight', 'old_decoder_blocks.15.attn.Wqkv.weight', 'gated_cross_attn_layers.21.attn_gate', 'gated_cross_attn_layers.15.ff_gate', 'old_decoder_blocks.3.attn.k_ln.weight', 'gated_cross_attn_layers.6.ff.0.bias', 'gated_cross_attn_layers.8.attn.norm.bias', 'gated_cross_attn_layers.16.attn.to_q.weight', 'gated_cross_attn_layers.18.ff.1.weight', 'old_decoder_blocks.6.attn.q_ln.weight', 'gated_cross_attn_layers.11.attn.to_q.weight', 'gated_cross_attn_layers.18.ff_gate', 'gated_cross_attn_layers.23.attn.to_kv.weight', 'gated_cross_attn_layers.14.ff.3.weight', 'gated_cross_attn_layers.0.attn.norm.bias', 'old_decoder_blocks.19.ln_2.weight', 'old_decoder_blocks.7.mlp.mlp_up.weight', 'old_decoder_blocks.9.mlp.mlp_down.weight', 'old_decoder_blocks.1.mlp.mlp_down.weight', 'gated_cross_attn_layers.3.ff.0.weight', 'gated_cross_attn_layers.5.ff_gate', 'gated_cross_attn_layers.22.attn.to_q.weight', 'old_decoder_blocks.6.attn.Wqkv.weight', 'gated_cross_attn_layers.15.attn_gate', 'old_decoder_blocks.8.attn.Wqkv.weight', 'gated_cross_attn_layers.22.attn.norm.bias', 'old_decoder_blocks.23.ln_2.weight', 'old_decoder_blocks.13.attn.k_ln.weight', 'gated_cross_attn_layers.6.ff.3.weight', 'old_decoder_blocks.22.attn.out_proj.weight', 'gated_cross_attn_layers.0.ff.3.weight', 'gated_cross_attn_layers.3.attn.norm.bias', 'gated_cross_attn_layers.4.ff.0.weight', 'gated_cross_attn_layers.0.attn.to_out.weight', 'gated_cross_attn_layers.1.ff_gate', 'old_decoder_blocks.16.mlp.mlp_down.weight', 'old_decoder_blocks.2.attn.out_proj.weight', 'old_decoder_blocks.12.attn.k_ln.weight', 'old_decoder_blocks.18.attn.out_proj.weight', 'gated_cross_attn_layers.9.attn.norm.weight', 'old_decoder_blocks.0.mlp.mlp_down.weight', 'gated_cross_attn_layers.21.ff.1.weight', 'old_decoder_blocks.21.ln_1.weight', 'old_decoder_blocks.22.attn.q_ln.weight', 'old_decoder_blocks.8.attn.out_proj.weight', 'gated_cross_attn_layers.4.ff.0.bias', 'gated_cross_attn_layers.5.attn.to_q.weight', 'gated_cross_attn_layers.22.attn.to_out.weight', 'old_decoder_blocks.7.attn.k_ln.weight', 'old_decoder_blocks.19.attn.out_proj.weight', 'old_decoder_blocks.7.attn.out_proj.weight', 'old_decoder_blocks.18.ln_2.weight', 'gated_cross_attn_layers.18.attn.norm.weight', 'gated_cross_attn_layers.23.ff.0.weight', 'gated_cross_attn_layers.22.ff.0.weight', 'gated_cross_attn_layers.20.ff.0.bias', 'gated_cross_attn_layers.16.attn_gate', 'old_decoder_blocks.8.attn.q_ln.weight', 'gated_cross_attn_layers.4.ff.1.weight', 'gated_cross_attn_layers.12.ff.1.weight', 'gated_cross_attn_layers.12.attn.to_q.weight', 'gated_cross_attn_layers.3.ff.0.bias', 'old_decoder_blocks.22.ln_1.weight', 'old_decoder_blocks.7.attn.Wqkv.weight', 'old_decoder_blocks.23.attn.k_ln.weight', 'old_decoder_blocks.13.mlp.mlp_up.weight', 'gated_cross_attn_layers.22.ff.1.weight', 'gated_cross_attn_layers.8.ff.0.weight', 'gated_cross_attn_layers.15.attn.norm.bias', 'old_decoder_blocks.7.ln_1.weight', 'old_decoder_blocks.21.mlp.mlp_down.weight', 'old_decoder_blocks.11.ln_2.weight', 'gated_cross_attn_layers.19.attn.norm.bias', 'gated_cross_attn_layers.21.ff.0.bias', 'old_decoder_blocks.1.attn.q_ln.weight', 'gated_cross_attn_layers.10.attn_gate', 'gated_cross_attn_layers.6.attn.norm.weight', 'old_decoder_blocks.2.ln_1.weight', 'gated_cross_attn_layers.14.ff.0.weight', 'old_decoder_blocks.11.attn.Wqkv.weight', 'old_decoder_blocks.3.attn.q_ln.weight', 'old_decoder_blocks.2.ln_2.weight', 'gated_cross_attn_layers.10.ff.1.weight', 'old_decoder_blocks.10.ln_2.weight', 'gated_cross_attn_layers.10.ff.0.bias', 'old_decoder_blocks.16.ln_2.weight', 'gated_cross_attn_layers.19.ff.1.weight', 'old_decoder_blocks.10.attn.k_ln.weight', 'old_decoder_blocks.9.ln_1.weight', 'old_decoder_blocks.15.ln_1.weight', 'old_decoder_blocks.2.attn.q_ln.weight', 'old_decoder_blocks.20.attn.q_ln.weight', 'old_decoder_blocks.12.attn.out_proj.weight', 'old_decoder_blocks.14.attn.k_ln.weight', 'old_decoder_blocks.20.mlp.mlp_up.weight', 'old_decoder_blocks.0.ln_2.weight', 'gated_cross_attn_layers.6.ff_gate', 'old_decoder_blocks.6.mlp.mlp_down.weight', 'old_decoder_blocks.13.attn.out_proj.weight', 'old_decoder_blocks.5.mlp.mlp_up.weight', 'old_decoder_blocks.9.attn.q_ln.weight', 'gated_cross_attn_layers.7.ff.3.weight', 'gated_cross_attn_layers.23.ff.0.bias', 'old_decoder_blocks.11.mlp.mlp_up.weight', 'old_decoder_blocks.13.ln_1.weight', 'old_decoder_blocks.20.attn.Wqkv.weight', 'old_decoder_blocks.4.attn.q_ln.weight', 'old_decoder_blocks.12.ln_1.weight', 'old_decoder_blocks.7.attn.q_ln.weight', 'old_decoder_blocks.12.mlp.mlp_up.weight', 'old_decoder_blocks.3.ln_2.weight', 'old_decoder_blocks.3.mlp.mlp_up.weight', 'gated_cross_attn_layers.17.ff.0.bias', 'old_decoder_blocks.10.mlp.mlp_down.weight', 'old_decoder_blocks.9.attn.k_ln.weight', 'old_decoder_blocks.1.ln_2.weight', 'old_decoder_blocks.1.attn.out_proj.weight', 'gated_cross_attn_layers.5.ff.1.weight', 'old_decoder_blocks.14.mlp.mlp_down.weight', 'old_decoder_blocks.21.attn.Wqkv.weight', 'gated_cross_attn_layers.9.ff.3.weight', 'gated_cross_attn_layers.7.attn.norm.weight', 'old_decoder_blocks.6.attn.k_ln.weight', 'old_decoder_blocks.18.attn.Wqkv.weight', 'old_decoder_blocks.23.attn.out_proj.weight', 'gated_cross_attn_layers.14.ff.1.weight', 'gated_cross_attn_layers.5.attn_gate', 'gated_cross_attn_layers.3.ff.1.weight', 'old_decoder_blocks.3.mlp.mlp_down.weight', 'gated_cross_attn_layers.7.attn.to_q.weight', 'gated_cross_attn_layers.3.attn.to_q.weight', 'old_decoder_blocks.3.ln_1.weight', 'gated_cross_attn_layers.2.attn.to_q.weight', 'old_decoder_blocks.2.mlp.mlp_down.weight', 'old_decoder_blocks.17.attn.q_ln.weight', 'gated_cross_attn_layers.13.ff.0.weight', 'gated_cross_attn_layers.17.attn.to_kv.weight', 'old_decoder_blocks.19.attn.Wqkv.weight', 'gated_cross_attn_layers.12.attn.to_kv.weight', 'old_decoder_blocks.4.mlp.mlp_down.weight', 'old_decoder_blocks.14.ln_1.weight', 'old_decoder_blocks.5.ln_2.weight', 'gated_cross_attn_layers.11.ff.0.bias', 'old_decoder_blocks.6.ln_1.weight', 'gated_cross_attn_layers.14.attn.to_q.weight', 'gated_cross_attn_layers.14.attn.to_out.weight', 'gated_cross_attn_layers.4.attn.to_q.weight', 'old_decoder_blocks.11.attn.q_ln.weight', 'old_decoder_blocks.16.attn.k_ln.weight', 'gated_cross_attn_layers.19.ff_gate', 'old_decoder_blocks.15.attn.k_ln.weight', 'gated_cross_attn_layers.5.attn.to_kv.weight', 'gated_cross_attn_layers.9.attn.to_kv.weight', 'old_decoder_blocks.16.attn.Wqkv.weight', 'old_decoder_blocks.15.mlp.mlp_up.weight', 'gated_cross_attn_layers.9.attn.to_out.weight', 'gated_cross_attn_layers.16.ff.0.bias', 'old_decoder_blocks.2.mlp.mlp_up.weight', 'gated_cross_attn_layers.2.attn_gate', 'gated_cross_attn_layers.10.attn.to_q.weight', 'old_decoder_blocks.0.attn.k_ln.weight', 'gated_cross_attn_layers.5.attn.norm.weight', 'gated_cross_attn_layers.19.attn.norm.weight', 'old_decoder_blocks.7.mlp.mlp_down.weight', 'old_decoder_blocks.4.attn.k_ln.weight', 'old_decoder_blocks.13.mlp.mlp_down.weight', 'gated_cross_attn_layers.5.ff.3.weight', 'gated_cross_attn_layers.1.ff.0.weight', 'old_decoder_blocks.15.attn.out_proj.weight', 'old_decoder_blocks.8.ln_2.weight', 'gated_cross_attn_layers.12.attn_gate', 'old_decoder_blocks.21.ln_2.weight', 'gated_cross_attn_layers.22.attn.norm.weight', 'old_decoder_blocks.17.ln_1.weight', 'gated_cross_attn_layers.15.attn.to_kv.weight', 'old_decoder_blocks.4.ln_2.weight', 'gated_cross_attn_layers.21.attn.to_kv.weight', 'old_decoder_blocks.9.mlp.mlp_up.weight', 'gated_cross_attn_layers.5.attn.norm.bias', 'gated_cross_attn_layers.21.attn.to_q.weight', 'old_decoder_blocks.17.attn.out_proj.weight', 'gated_cross_attn_layers.10.ff.0.weight', 'old_decoder_blocks.13.ln_2.weight', 'gated_cross_attn_layers.17.ff.1.weight', 'old_decoder_blocks.14.attn.Wqkv.weight', 'gated_cross_attn_layers.10.ff.3.weight', 'gated_cross_attn_layers.0.ff.0.bias', 'gated_cross_attn_layers.23.attn.norm.bias', 'old_decoder_blocks.10.attn.out_proj.weight', 'gated_cross_attn_layers.19.attn.to_q.weight', 'gated_cross_attn_layers.2.attn.norm.bias', 'old_decoder_blocks.11.ln_1.weight', 'gated_cross_attn_layers.18.attn.to_out.weight', 'gated_cross_attn_layers.9.ff.0.bias', 'gated_cross_attn_layers.15.ff.0.bias', 'gated_cross_attn_layers.8.ff.1.weight', 'gated_cross_attn_layers.2.attn.to_kv.weight', 'old_decoder_blocks.6.attn.out_proj.weight', 'old_decoder_blocks.14.ln_2.weight', 'gated_cross_attn_layers.0.attn.to_q.weight', 'old_decoder_blocks.9.ln_2.weight', 'old_decoder_blocks.4.attn.Wqkv.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    }
   ],
   "source": [
    "model.model.lang_encoder.save_pretrained(\"/mnt/d/models/OpenFlamingo-3B-vitl-mpt1b-ft-squad/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0aee2ca9c414985834cd5e932b63870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.38G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ToviTu/fine-tuned-nl-flamingo/commit/51d58c5fd57ca92f5bcdd7d65afd0b0a2f1e3c72', commit_message='Upload MosaicGPT', commit_description='', oid='51d58c5fd57ca92f5bcdd7d65afd0b0a2f1e3c72', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.lang_encoder.push_to_hub(\"ToviTu/fine-tuned-nl-flamingo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flamingo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
