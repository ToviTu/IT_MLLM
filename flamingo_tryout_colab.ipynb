{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2k0MmthXpIa",
    "outputId": "02e15313-fcd7-4819-e43d-da1b6bdf361b"
   },
   "outputs": [],
   "source": [
    "import open_flamingo\n",
    "from open_flamingo import create_model_and_transforms\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RHsQxr8cZJGa",
    "outputId": "8c8ec826-ef46-4782-863c-6386f09ae707"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/huggingface/modules/transformers_modules/anas-awadalla/mpt-1b-redpajama-200b-dolly/f0a13e41fcee2217cd701219ffa1eaef7fe955ea/attention.py:289: UserWarning: Using `attn_impl: torch`. If your model does not use `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n",
      "Flamingo model initialized with 1046992944 trainable parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Flamingo(\n",
       "  (vision_encoder): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-23): 24 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (perceiver): PerceiverResampler(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x ModuleList(\n",
       "        (0): PerceiverAttention(\n",
       "          (norm_media): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm_latents): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=1024, bias=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lang_encoder): MosaicGPT(\n",
       "    (transformer): ModuleDict(\n",
       "      (wte): Embedding(50280, 2048)\n",
       "      (emb_drop): Dropout(p=0, inplace=False)\n",
       "      (blocks): ModuleList(\n",
       "        (0-23): 24 x FlamingoLayer(\n",
       "          (gated_cross_attn_layer): GatedCrossAttentionBlock(\n",
       "            (attn): MaskedCrossAttention(\n",
       "              (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): GPTBlock(\n",
       "            (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "              (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "              (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "              (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPTMLP(\n",
       "              (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "              (mlp_act): GELU(approximate='none')\n",
       "              (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (old_decoder_blocks): ModuleList(\n",
       "      (0-23): 24 x GPTBlock(\n",
       "        (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTMLP(\n",
       "          (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (mlp_act): GELU(approximate='none')\n",
       "          (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (gated_cross_attn_layers): ModuleList(\n",
       "      (0-23): 24 x GatedCrossAttentionBlock(\n",
       "        (attn): MaskedCrossAttention(\n",
       "          (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=2048, bias=False)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, image_processor, tokenizer = create_model_and_transforms(\n",
    "    clip_vision_encoder_path=\"ViT-L-14\",\n",
    "    clip_vision_encoder_pretrained=\"openai\",\n",
    "    lang_encoder_path=\"anas-awadalla/mpt-1b-redpajama-200b-dolly\",\n",
    "    tokenizer_path=\"anas-awadalla/mpt-1b-redpajama-200b-dolly\",\n",
    "    cross_attn_every_n_layers=1,\n",
    "    )\n",
    "\n",
    "model.to(0, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "di6XlpdvZfLT"
   },
   "outputs": [],
   "source": [
    "def qa_prompt(question, answer=None) -> str:\n",
    "        return f\"<image>Question:{question} Short answer:{answer if answer is not None else ''}{'<|endofchunk|>' if answer is not None else ''}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_image_url = lambda url: Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "def image_preprocess_batch(images: list)->torch.Tensor:\n",
    "    vision_x = [image_processor(image).unsqueeze(0) for image in images]\n",
    "    vision_x = torch.cat(vision_x, dim=0)\n",
    "    vision_x = vision_x.unsqueeze(1).unsqueeze(0)\n",
    "    return vision_x.cuda()\n",
    "\n",
    "def text_processor_factory(tokenizer, padding_side):\n",
    "    tokenizer.padding_side = padding_side\n",
    "    return lambda text: tokenizer([text], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def make_in_context(d, shots=1):\n",
    "    images = []\n",
    "    prompt = ''\n",
    "    for _ in range(shots):\n",
    "        idx = int(random.random() * len(d))\n",
    "        image = d[idx]['image']\n",
    "        question = d[idx]['question']\n",
    "        answers = d[idx]['answers'][0]\n",
    "\n",
    "        images.append(image)\n",
    "        prompt += qa_prompt(question, answers) + \"<|endofchunk|>\" +'\\n'\n",
    "        if shots == 0:\n",
    "                prompt = prompt.replace(\"<image>\", \"\")\n",
    "    return images, prompt\n",
    "\n",
    "def postprocess_vqa_generation(predictions):\n",
    "    answer = re.split(\"Question|Answer|Short\", predictions, 1)[0]\n",
    "    answer = re.split(\", \", answer, 1)[0]\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, tqdm, re\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def vqa_evaluate(model, tokenizer, dataset, early_stop=None, shots=1):\n",
    "    model.eval()\n",
    "    if early_stop is None:\n",
    "        early_stop = len(dataset)\n",
    "    results = []\n",
    "    counter = 0\n",
    "    for q in tqdm.tqdm(dataset):\n",
    "        counter += 1\n",
    "        if counter > early_stop:\n",
    "            break\n",
    "\n",
    "        image_in_context, text_in_context = make_in_context(dataset, shots=shots)\n",
    "\n",
    "        image = q['image']\n",
    "        qestion = q['question']\n",
    "        answers = q['answers']\n",
    "        image_tokens = image_preprocess_batch(image_in_context + [image])\n",
    "        text_tokens = text_processor_factory(tokenizer, \"left\")(text_in_context+qa_prompt(qestion))\n",
    "\n",
    "        #print(text_in_context+qa_prompt(qestion))\n",
    "\n",
    "        output_tokens = model.generate(\n",
    "            image_tokens.to(0, dtype=torch.bfloat16),\n",
    "            text_tokens['input_ids'].to(0),\n",
    "            attention_mask=text_tokens['attention_mask'].to(0, dtype=torch.bfloat16),\n",
    "            max_new_tokens=5,\n",
    "            num_beams=3,\n",
    "            pad_token_id=50277\n",
    "            )\n",
    "        outputs = output_tokens[:, len(text_tokens['input_ids'][0]) :]\n",
    "\n",
    "        y_hat = tokenizer.decode(outputs[0])\n",
    "        y_hat = postprocess_vqa_generation(y_hat)\n",
    "\n",
    "        results.append({\n",
    "            \"answer\": y_hat,\n",
    "            \"question_id\": q[\"question_id\"]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def results2json(results, name=''):\n",
    "    json.dump(results, open(f'{VQA_DATA_DIR}{name}results.json', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                   | 0/214354 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataParallel' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 12\u001b[0m\n\u001b[1;32m      3\u001b[0m VQA_DATA_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/external/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m d \u001b[38;5;241m=\u001b[39m VQADataset(\n\u001b[1;32m      6\u001b[0m     VQA_DATA_DIR \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval2014\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     VQA_DATA_DIR \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv2_OpenEnded_mscoco_val2014_questions.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvqav2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mvqa_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m results2json(results, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvqa_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m, in \u001b[0;36mvqa_evaluate\u001b[0;34m(model, tokenizer, dataset, early_stop, shots)\u001b[0m\n\u001b[1;32m     21\u001b[0m text_tokens \u001b[38;5;241m=\u001b[39m text_processor_factory(tokenizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)(text_in_context\u001b[38;5;241m+\u001b[39mqa_prompt(qestion))\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#print(text_in_context+qa_prompt(qestion))\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m(\n\u001b[1;32m     26\u001b[0m     image_tokens\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16),\n\u001b[1;32m     27\u001b[0m     text_tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     28\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mtext_tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16),\n\u001b[1;32m     29\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     30\u001b[0m     num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     31\u001b[0m     pad_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50277\u001b[39m\n\u001b[1;32m     32\u001b[0m     )\n\u001b[1;32m     33\u001b[0m outputs \u001b[38;5;241m=\u001b[39m output_tokens[:, \u001b[38;5;28mlen\u001b[39m(text_tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]) :]\n\u001b[1;32m     35\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataParallel' object has no attribute 'generate'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m(1614)\u001b[0;36m__getattr__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   1612 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1613 \u001b[0;31m                \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m-> 1614 \u001b[0;31m        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
      "\u001b[0m\u001b[0;32m   1615 \u001b[0;31m            type(self).__name__, name))\n",
      "\u001b[0m\u001b[0;32m   1616 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  u\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_2270/1512339583.py\u001b[0m(25)\u001b[0;36mvqa_evaluate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     23 \u001b[0;31m        \u001b[0;31m#print(text_in_context+qa_prompt(qestion))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     24 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 25 \u001b[0;31m        output_tokens = model.generate(\n",
      "\u001b[0m\u001b[0;32m     26 \u001b[0;31m            \u001b[0mimage_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     27 \u001b[0;31m            \u001b[0mtext_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): Flamingo(\n",
      "    (vision_encoder): VisionTransformer(\n",
      "      (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "      (patch_dropout): Identity()\n",
      "      (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (transformer): Transformer(\n",
      "        (resblocks): ModuleList(\n",
      "          (0-23): 24 x ResidualAttentionBlock(\n",
      "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ls_1): Identity()\n",
      "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Sequential(\n",
      "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (gelu): QuickGELU()\n",
      "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (ls_2): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (perceiver): PerceiverResampler(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x ModuleList(\n",
      "          (0): PerceiverAttention(\n",
      "            (norm_media): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_latents): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (to_q): Linear(in_features=1024, out_features=512, bias=False)\n",
      "            (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (to_out): Linear(in_features=512, out_features=1024, bias=False)\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "            (2): GELU(approximate='none')\n",
      "            (3): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lang_encoder): MosaicGPT(\n",
      "      (transformer): ModuleDict(\n",
      "        (wte): Embedding(50280, 2048)\n",
      "        (emb_drop): Dropout(p=0, inplace=False)\n",
      "        (blocks): ModuleList(\n",
      "          (0-23): 24 x FlamingoLayer(\n",
      "            (gated_cross_attn_layer): GatedCrossAttentionBlock(\n",
      "              (attn): MaskedCrossAttention(\n",
      "                (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "                (to_q): Linear(in_features=2048, out_features=512, bias=False)\n",
      "                (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                (to_out): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              )\n",
      "              (ff): Sequential(\n",
      "                (0): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "                (1): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "                (2): GELU(approximate='none')\n",
      "                (3): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (decoder_layer): GPTBlock(\n",
      "              (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): MultiheadAttention(\n",
      "                (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
      "                (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "                (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "                (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              )\n",
      "              (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): GPTMLP(\n",
      "                (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "                (mlp_act): GELU(approximate='none')\n",
      "                (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "              )\n",
      "              (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
      "              (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (old_decoder_blocks): ModuleList(\n",
      "        (0-23): 24 x GPTBlock(\n",
      "          (ln_1): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): MultiheadAttention(\n",
      "            (Wqkv): Linear(in_features=2048, out_features=6144, bias=False)\n",
      "            (q_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "            (k_ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "            (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          )\n",
      "          (ln_2): LPLayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPTMLP(\n",
      "            (mlp_up): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "            (mlp_act): GELU(approximate='none')\n",
      "            (mlp_down): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          )\n",
      "          (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_mlp_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (gated_cross_attn_layers): ModuleList(\n",
      "        (0-23): 24 x GatedCrossAttentionBlock(\n",
      "          (attn): MaskedCrossAttention(\n",
      "            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "            (to_q): Linear(in_features=2048, out_features=512, bias=False)\n",
      "            (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (to_out): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          )\n",
      "          (ff): Sequential(\n",
      "            (0): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "            (1): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "            (2): GELU(approximate='none')\n",
      "            (3): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  dir(model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'cpu', 'cuda', 'device_ids', 'dim', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'gather', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'module', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'output_device', 'parallel_apply', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'replicate', 'requires_grad_', 'scatter', 'set_extra_state', 'share_memory', 'src_device_obj', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    }
   ],
   "source": [
    "from open_flamingo.eval.eval_datasets import VQADataset\n",
    "\n",
    "VQA_DATA_DIR = \"/external/\"\n",
    "\n",
    "d = VQADataset(\n",
    "    VQA_DATA_DIR + 'val2014',\n",
    "    VQA_DATA_DIR + 'v2_OpenEnded_mscoco_val2014_questions.json',\n",
    "    VQA_DATA_DIR + 'v2_mscoco_val2014_annotations.json',\n",
    "    is_train = False,\n",
    "    dataset_name = 'vqav2'\n",
    ")\n",
    "results = vqa_evaluate(model, tokenizer, d, 100, shots=4)\n",
    "results2json(results, 'vqa_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VQA annotations and questions into memory...\n",
      "0:00:06.233405\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...     \n",
      "DONE (t=0.13s)\n",
      "creating index...\n",
      "index created!\n",
      "computing accuracy\n",
      "Finshed Percent: [--------------------] 0% Done computing accuracy\n",
      "26.1\n"
     ]
    }
   ],
   "source": [
    "from open_flamingo.eval.vqa_metric import VQA, VQAEval\n",
    "\n",
    "v = VQA(\n",
    "    f\"{VQA_DATA_DIR}v2_mscoco_val2014_annotations.json\",\n",
    "    f\"{VQA_DATA_DIR}v2_OpenEnded_mscoco_val2014_questions.json\",\n",
    ")\n",
    "res = v.loadRes(f'{VQA_DATA_DIR}vqa_results.json', f\"{VQA_DATA_DIR}v2_OpenEnded_mscoco_val2014_questions.json\")\n",
    "\n",
    "# create evaluator\n",
    "evaluator = VQAEval(v, res)\n",
    "evaluator.evaluate()\n",
    "print(evaluator.accuracy['overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM54bAGKZSCRsGZFzoZooFW",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ba9a4dbc6ba4d9ba3bce384b0b5a961": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1de63947f83a451dabf1adabad521adc",
      "placeholder": "​",
      "style": "IPY_MODEL_916ab2875bfe4e15983050577d352ec7",
      "value": " 4.19G/4.19G [00:19&lt;00:00, 256MB/s]"
     }
    },
    "1224601271884f2a9260fe2349df9c01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1a45a399dbe841949075e76ba5048ef6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1a5e7bd931254f68909f2c814ec9c073": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be7df70169ba4075a1ba5b1c3a7a6d01",
      "placeholder": "​",
      "style": "IPY_MODEL_ed3446b86bce4869bfc685ae58266d70",
      "value": "checkpoint.pt: 100%"
     }
    },
    "1de63947f83a451dabf1adabad521adc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23dbc31b57c6492a8b074bc8119cddf4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35ea935c05344c43b706e632c557008c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "498b93ad93c14ef39aa402e52deefd7a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62de1c57a73f4a5789da8c1c42a0c340": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1a5e7bd931254f68909f2c814ec9c073",
       "IPY_MODEL_ce63ba97e19e4ae9ae3ff9fbbb0c42c8",
       "IPY_MODEL_9339a61d8f374501866829e8593c0dee"
      ],
      "layout": "IPY_MODEL_23dbc31b57c6492a8b074bc8119cddf4"
     }
    },
    "6e5f9405d61c46349e12d5f8474b42fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "738aaf1e288542ecbd2df9c5b7b73f1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7a1d5e37d95c4026964d4ca2b19ae524": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f079869bb8f482b8dc33a32e05a300b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_89ee76fa71644ef48aa3f42e6279abfb",
       "IPY_MODEL_88001ba471f14d848f9c29d9861a07d6",
       "IPY_MODEL_0ba9a4dbc6ba4d9ba3bce384b0b5a961"
      ],
      "layout": "IPY_MODEL_c823ba48620642608a2e1cb794e7db70"
     }
    },
    "88001ba471f14d848f9c29d9861a07d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0878437ef2b41c59a91bed94bcffc98",
      "max": 4188086933,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_35ea935c05344c43b706e632c557008c",
      "value": 4188086933
     }
    },
    "89ee76fa71644ef48aa3f42e6279abfb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e5f9405d61c46349e12d5f8474b42fb",
      "placeholder": "​",
      "style": "IPY_MODEL_1224601271884f2a9260fe2349df9c01",
      "value": "checkpoint.pt: 100%"
     }
    },
    "916ab2875bfe4e15983050577d352ec7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9339a61d8f374501866829e8593c0dee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a1d5e37d95c4026964d4ca2b19ae524",
      "placeholder": "​",
      "style": "IPY_MODEL_738aaf1e288542ecbd2df9c5b7b73f1a",
      "value": " 4.19G/4.19G [00:30&lt;00:00, 241MB/s]"
     }
    },
    "be7df70169ba4075a1ba5b1c3a7a6d01": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c823ba48620642608a2e1cb794e7db70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce63ba97e19e4ae9ae3ff9fbbb0c42c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_498b93ad93c14ef39aa402e52deefd7a",
      "max": 4188086933,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1a45a399dbe841949075e76ba5048ef6",
      "value": 4188086933
     }
    },
    "e0878437ef2b41c59a91bed94bcffc98": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed3446b86bce4869bfc685ae58266d70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
