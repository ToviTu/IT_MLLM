{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.model import EvalModel\n",
    "import os\n",
    "from scripts.datasets import SQUAD_dataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = (\n",
    "    \"/mnt/d/models/\"\n",
    "    if os.environ.get(\"CHECKPOINT_DIR\") == None\n",
    "    else os.environ[\"CHECKPOINT_DIR\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Checkpoint from /mnt/d/models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tovitu/.cache/huggingface/modules/transformers_modules/anas-awadalla/mpt-1b-redpajama-200b/50d6bc94e17812873f39c36c5f815263fa71fb80/attention.py:289: UserWarning: Using `attn_impl: torch`. If your model does not use `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n",
      "Flamingo model initialized with 1046992944 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model_args = {\n",
    "    \"vision_encoder_path\": \"ViT-L-14\",\n",
    "    \"vision_encoder_pretrained\": \"openai\",\n",
    "    \"lm_path\": \"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "    \"lm_tokenizer_path\": \"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "    \"checkpoint_path\": f\"{CHECKPOINT_DIR}/OpenFlamingo-3B-vitl-mpt1b/checkpoint.pt\",\n",
    "    \"cross_attn_every_n_layers\": 1,\n",
    "    \"precision\": \"bf16\",\n",
    "    \"device\": 0,\n",
    "}\n",
    "\n",
    "print(f\"Loading Checkpoint from {CHECKPOINT_DIR}\")\n",
    "model = EvalModel(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SQUAD_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1046992944 || all params: 2559117360 || trainable%: 40.91\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 68419584 || all params: 2627536944 || trainable%: 2.60\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r = 256,\n",
    "    lora_alpha = 512,\n",
    "    target_modules = ['Wqkv', 'to_q', 'to_k', 'to_v'],\n",
    "    lora_dropout=0.1,\n",
    "    bias='none',\n",
    ")\n",
    "lora_model = get_peft_model(model.model, config)\n",
    "print_trainable_parameters(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/mnt/d/datasets/psuedo_dataset.json', 'r') as f:\n",
    "    pseudo_outputs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = '''\n",
    "Suppose you are teaching a first grader. Answer the question according to the provided context and explain to the first grader your method to find the answer without referring back to the context in the form \"Rationale: <reasons> \\n Answer: <answer>\" where <reasons> and <answer> are your response. Make your response as short as possible.\\n\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.uncache_media()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 100/1974 [00:26<07:27,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.25265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 200/1974 [00:49<06:54,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.2352734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 300/1974 [01:12<06:17,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.2245572916666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 400/1974 [01:36<06:26,  4.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.22423828125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 500/1974 [02:02<05:41,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.20275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 600/1974 [02:25<05:13,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.1969010416666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 700/1974 [02:47<04:40,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.2007924107142856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 800/1974 [03:10<04:32,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.201474609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 900/1974 [03:33<04:12,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.2047829861111112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 1000/1974 [03:57<03:49,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.202515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 1100/1974 [04:19<03:22,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.1987642045454545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 1200/1974 [04:41<03:01,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.1999479166666664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 1300/1974 [05:04<02:37,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.1992608173076924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1400/1974 [05:28<02:06,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.196434151785714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 1500/1974 [05:50<01:48,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.193619791666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 1600/1974 [06:12<01:28,  4.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.1916552734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 1700/1974 [06:35<01:05,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.193809742647059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 1800/1974 [06:58<00:40,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.195842013888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 1900/1974 [07:22<00:17,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.1938034539473685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1974/1974 [07:39<00:00,  4.30it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.model.parameters(), lr=2e-4, weight_decay=0.01)\n",
    "count = 0\n",
    "\n",
    "media_token_id = model.tokenizer(\"<image>\", add_special_tokens=False)[\"input_ids\"][-1]\n",
    "endofchunk_token_id = model.tokenizer(\"<|endofchunk|>\", add_special_tokens=False)[\n",
    "    \"input_ids\"\n",
    "][-1]\n",
    "lora_model.train()\n",
    "\n",
    "loss_vals = []\n",
    "for epoch in range(1):\n",
    "    counter = 0\n",
    "    for key, batch in tqdm.tqdm(pseudo_outputs.items()):\n",
    "\n",
    "        batch = \"<image>\" + batch[len(instruction)-2:] +\"<|endofchunk|>\"\n",
    "\n",
    "        token = model.tokenizer(batch)\n",
    "        image = Image.open(f'/mnt/d/datasets/images/{key}.png')\n",
    "        image_token = data.image_preprocess_batch(model.image_processor, [image])\n",
    "        #image_tokens = torch.cat([image_token], dim=0)\n",
    "        input_ids, attention_mask = model._prepare_text(batch)\n",
    "\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == model.tokenizer.pad_token_id] = -100\n",
    "        labels[labels == media_token_id] = -100\n",
    "\n",
    "        loss = lora_model(\n",
    "                    image_token.to(0, dtype=torch.bfloat16),\n",
    "                    input_ids,\n",
    "                    attention_mask,\n",
    "                    labels = labels\n",
    "                )[0]\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(lora_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        loss_vals.append(loss.cpu().detach().cpu().item())\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(f\"loss: {sum(loss_vals)/len(loss_vals)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MAX_LEN'] = \"20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2000 [00:01<1:01:33,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "denver broncos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/2000 [00:03<55:11,  1.66s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "denver broncos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/2000 [00:05<1:02:06,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "denver broncos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/2000 [00:06<55:38,  1.67s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "denver broncos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/2000 [00:12<1:41:24,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "golden anniversary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/2000 [00:18<2:12:39,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theme of super bowl 50 was golden anniversary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/2000 [00:21<2:04:32,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "february 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/2000 [00:26<2:23:35,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "american football conference afc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/2000 [00:30<2:08:27,  3.87s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/fine_tune.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/fine_tune.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m data\u001b[39m.\u001b[39;49minfer(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/fine_tune.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     model\u001b[39m.\u001b[39;49mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/fine_tune.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     model\u001b[39m.\u001b[39;49mimage_processor,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/fine_tune.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     model\u001b[39m.\u001b[39;49mtokenizer,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/fine_tune.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39m/mnt/d/datasets/\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/fine_tune.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     early_stop\u001b[39m=\u001b[39;49m\u001b[39m2000\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/fine_tune.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     shots \u001b[39m=\u001b[39;49m \u001b[39m4\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/fine_tune.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "File \u001b[0;32m~/codes/Instruction-tuned-Flamingo-MLLM/scripts/datasets.py:391\u001b[0m, in \u001b[0;36mSQUAD_dataset.infer\u001b[0;34m(self, model, image_encoder, text_encoder, output_dir, shots, early_stop)\u001b[0m\n\u001b[1;32m    388\u001b[0m     answer_len \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mMAX_LEN\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    390\u001b[0m \u001b[39m# Inference\u001b[39;00m\n\u001b[0;32m--> 391\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    392\u001b[0m     image_token\u001b[39m.\u001b[39;49mto(device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice, dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision),\n\u001b[1;32m    393\u001b[0m     text_token[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice),\n\u001b[1;32m    394\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mtext_token[\u001b[39m\"\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mto(\n\u001b[1;32m    395\u001b[0m         device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice, dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision\n\u001b[1;32m    396\u001b[0m     ),\n\u001b[1;32m    397\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49manswer_len,\n\u001b[1;32m    398\u001b[0m     num_beams\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m    399\u001b[0m     pad_token_id\u001b[39m=\u001b[39;49m\u001b[39m50277\u001b[39;49m,\n\u001b[1;32m    400\u001b[0m )\n\u001b[1;32m    401\u001b[0m output \u001b[39m=\u001b[39m text_encoder\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m])\n\u001b[1;32m    402\u001b[0m output \u001b[39m=\u001b[39m output[\u001b[39mlen\u001b[39m(prompt) :]\n",
      "File \u001b[0;32m~/codes/Instruction-tuned-Flamingo-MLLM/open_flamingo/src/flamingo.py:165\u001b[0m, in \u001b[0;36mFlamingo.generate\u001b[0;34m(self, vision_x, lang_x, attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_vision_x(vision_x\u001b[39m=\u001b[39mvision_x)\n\u001b[1;32m    164\u001b[0m eos_token_id \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39meos_token_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meoc_token_id)\n\u001b[0;32m--> 165\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlang_encoder\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    166\u001b[0m     input_ids\u001b[39m=\u001b[39;49mlang_x,\n\u001b[1;32m    167\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    168\u001b[0m     eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m    169\u001b[0m     num_beams\u001b[39m=\u001b[39;49mnum_beams,\n\u001b[1;32m    170\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    171\u001b[0m )\n\u001b[1;32m    173\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlang_encoder\u001b[39m.\u001b[39mclear_conditioned_layers()\n\u001b[1;32m    174\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlang_encoder\u001b[39m.\u001b[39m_use_cached_vision_x \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/flamingo/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/flamingo/lib/python3.10/site-packages/transformers/generation/utils.py:1752\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1746\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1747\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1748\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1749\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1750\u001b[0m     )\n\u001b[1;32m   1751\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1752\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   1753\u001b[0m         input_ids,\n\u001b[1;32m   1754\u001b[0m         beam_scorer,\n\u001b[1;32m   1755\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1756\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1757\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1758\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1759\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1760\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1761\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1762\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1763\u001b[0m     )\n\u001b[1;32m   1765\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE:\n\u001b[1;32m   1766\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/miniconda3/envs/flamingo/lib/python3.10/site-packages/transformers/generation/utils.py:3091\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3087\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   3089\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 3091\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   3092\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   3093\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   3094\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   3095\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   3096\u001b[0m )\n\u001b[1;32m   3098\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3099\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/flamingo/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/codes/Instruction-tuned-Flamingo-MLLM/open_flamingo/src/flamingo_lm.py:157\u001b[0m, in \u001b[0;36mFlamingoLMMixin.forward\u001b[0;34m(self, input_ids, attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m input_ids\n\u001b[1;32m    156\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m attention_mask\n\u001b[0;32m--> 157\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/anas-awadalla/mpt-1b-redpajama-200b/50d6bc94e17812873f39c36c5f815263fa71fb80/mosaic_gpt.py:353\u001b[0m, in \u001b[0;36mMosaicGPT.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, labels, prefix_mask, sequence_id, return_dict, output_attentions, output_hidden_states, use_cache)\u001b[0m\n\u001b[1;32m    350\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (x,)\n\u001b[1;32m    351\u001b[0m past_key_value \u001b[39m=\u001b[39m past_key_values[\n\u001b[1;32m    352\u001b[0m     b_idx] \u001b[39mif\u001b[39;00m past_key_values \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m x, past_key_value \u001b[39m=\u001b[39m block(x,\n\u001b[1;32m    354\u001b[0m                           past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    355\u001b[0m                           attn_bias\u001b[39m=\u001b[39;49mattn_bias,\n\u001b[1;32m    356\u001b[0m                           attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    357\u001b[0m                           is_causal\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mis_causal)\n\u001b[1;32m    358\u001b[0m \u001b[39mif\u001b[39;00m past_key_values \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     past_key_values[b_idx] \u001b[39m=\u001b[39m past_key_value\n",
      "File \u001b[0;32m~/miniconda3/envs/flamingo/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/codes/Instruction-tuned-Flamingo-MLLM/open_flamingo/src/flamingo_lm.py:55\u001b[0m, in \u001b[0;36mFlamingoLayer.forward\u001b[0;34m(self, lang_x, attention_mask, **decoder_layer_kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmedia_locations \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     52\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmedia_locations must be conditioned before forward pass\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m         )\n\u001b[0;32m---> 55\u001b[0m     lang_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgated_cross_attn_layer(\n\u001b[1;32m     56\u001b[0m         lang_x,\n\u001b[1;32m     57\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvis_x,\n\u001b[1;32m     58\u001b[0m         media_locations\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmedia_locations,\n\u001b[1;32m     59\u001b[0m         use_cached_media\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_cached_media,\n\u001b[1;32m     60\u001b[0m     )\n\u001b[1;32m     62\u001b[0m \u001b[39m# Normal decoder layer\u001b[39;00m\n\u001b[1;32m     63\u001b[0m lang_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder_layer(\n\u001b[1;32m     64\u001b[0m     lang_x, attention_mask\u001b[39m=\u001b[39mattention_mask, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdecoder_layer_kwargs\n\u001b[1;32m     65\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/flamingo/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/codes/Instruction-tuned-Flamingo-MLLM/open_flamingo/src/helpers.py:268\u001b[0m, in \u001b[0;36mGatedCrossAttentionBlock.forward\u001b[0;34m(self, x, media, media_locations, use_cached_media)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    261\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    262\u001b[0m     x,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m     use_cached_media\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    266\u001b[0m ):\n\u001b[1;32m    267\u001b[0m     x \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 268\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    269\u001b[0m             x,\n\u001b[1;32m    270\u001b[0m             media,\n\u001b[1;32m    271\u001b[0m             media_locations\u001b[39m=\u001b[39;49mmedia_locations,\n\u001b[1;32m    272\u001b[0m             use_cached_media\u001b[39m=\u001b[39;49muse_cached_media,\n\u001b[1;32m    273\u001b[0m         )\n\u001b[1;32m    274\u001b[0m         \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_gate\u001b[39m.\u001b[39mtanh()\n\u001b[1;32m    275\u001b[0m         \u001b[39m+\u001b[39m x\n\u001b[1;32m    276\u001b[0m     )\n\u001b[1;32m    277\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff(x) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff_gate\u001b[39m.\u001b[39mtanh() \u001b[39m+\u001b[39m x\n\u001b[1;32m    279\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/flamingo/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/codes/Instruction-tuned-Flamingo-MLLM/open_flamingo/src/helpers.py:229\u001b[0m, in \u001b[0;36mMaskedCrossAttention.forward\u001b[0;34m(self, x, media, media_locations, use_cached_media)\u001b[0m\n\u001b[1;32m    225\u001b[0m     text_without_media_mask \u001b[39m=\u001b[39m text_time \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    226\u001b[0m     text_without_media_mask \u001b[39m=\u001b[39m rearrange(\n\u001b[1;32m    227\u001b[0m         text_without_media_mask, \u001b[39m\"\u001b[39m\u001b[39mb i -> b 1 i 1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    228\u001b[0m     )\n\u001b[0;32m--> 229\u001b[0m     attn \u001b[39m=\u001b[39m attn\u001b[39m.\u001b[39;49mmasked_fill(text_without_media_mask, \u001b[39m0.0\u001b[39;49m)\n\u001b[1;32m    231\u001b[0m out \u001b[39m=\u001b[39m einsum(\u001b[39m\"\u001b[39m\u001b[39m... i j, ... j d -> ... i d\u001b[39m\u001b[39m\"\u001b[39m, attn, v)\n\u001b[1;32m    232\u001b[0m out \u001b[39m=\u001b[39m rearrange(out, \u001b[39m\"\u001b[39m\u001b[39mb h n d -> b n (h d)\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data.infer(\n",
    "    model.model,\n",
    "    model.image_processor,\n",
    "    model.tokenizer,\n",
    "    '/mnt/d/datasets/',\n",
    "    early_stop=2000,\n",
    "    shots = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 17.0, 'f1': 31.420929103073792}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from evaluate import load\n",
    "\n",
    "DATASET_DIR = \"/mnt/d/datasets/\"\n",
    "\n",
    "with open(f\"{DATASET_DIR}squad_resutls.json\", \"r\") as f:\n",
    "    predictions = json.load(f)\n",
    "with open(f\"{DATASET_DIR}squad_references.json\", \"r\") as f:\n",
    "    references = json.load(f)\n",
    "\n",
    "squad_metric = load(\"squad\")\n",
    "results = squad_metric.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = lora_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(merged.state_dict(), '/mnt/d/models/fine-tuned-nl-flamingo-visual/checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sleeping for less than 5 hours is not recommended. Sleeping for less than 7 hours is not'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Sleeping for less than 5 hours is\"\n",
    "image = data.palceholder_image\n",
    "image_token = data.image_preprocess_batch(model.image_processor, [image])\n",
    "\n",
    "input_ids, attention_mask = model._prepare_text(text)\n",
    "output = model.model.generate(\n",
    "    image_token.to(0, dtype=torch.bfloat16),\n",
    "    input_ids,\n",
    "    attention_mask,\n",
    "    max_length=20,\n",
    "    num_beams=3,\n",
    "    pad_token_id=50277,\n",
    ")\n",
    "answer = model.tokenizer.decode(output[0])\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flamingo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
