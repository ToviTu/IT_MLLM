{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.model import EvalModel\n",
    "import os\n",
    "from scripts.datasets import SQUAD_dataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = (\n",
    "    \"/mnt/d/models/\"\n",
    "    if os.environ.get(\"CHECKPOINT_DIR\") == None\n",
    "    else os.environ[\"CHECKPOINT_DIR\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Checkpoint from /mnt/d/models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tovitu/.cache/huggingface/modules/transformers_modules/anas-awadalla/mpt-1b-redpajama-200b/50d6bc94e17812873f39c36c5f815263fa71fb80/attention.py:289: UserWarning: Using `attn_impl: torch`. If your model does not use `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n",
      "Flamingo model initialized with 1046992944 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model_args = {\n",
    "    \"vision_encoder_path\": \"ViT-L-14\",\n",
    "    \"vision_encoder_pretrained\": \"openai\",\n",
    "    \"lm_path\": \"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "    \"lm_tokenizer_path\": \"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "    \"checkpoint_path\": f\"{CHECKPOINT_DIR}/OpenFlamingo-3B-vitl-mpt1b/checkpoint.pt\",\n",
    "    \"cross_attn_every_n_layers\": 1,\n",
    "    \"precision\": \"bf16\",\n",
    "    \"device\": 0,\n",
    "}\n",
    "\n",
    "print(f\"Loading Checkpoint from {CHECKPOINT_DIR}\")\n",
    "model = EvalModel(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SQUAD_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1046992944 || all params: 2559117360 || trainable%: 40.91\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6291456 || all params: 2565408816 || trainable%: 0.25\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r = 32,\n",
    "    lora_alpha = 16,\n",
    "    target_modules = ['Wqkv'],\n",
    "    lora_dropout=0.1,\n",
    "    bias='none',\n",
    ")\n",
    "lora_model = get_peft_model(model.model, config)\n",
    "print_trainable_parameters(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/mnt/d/datasets/psuedo_dataset.json', 'r') as f:\n",
    "    pseudo_outputs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = '''\n",
    "Suppose you are teaching a first grader. Answer the question according to the provided context and explain to the first grader your method to find the answer without referring back to the context in the form \"Rationale: <reasons> \\n Answer: <answer>\" where <reasons> and <answer> are your response. Make your response as short as possible.\\n\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.uncache_media()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 461/1974 [02:40<08:48,  2.86it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/fine_tune.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/fine_tune.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/fine_tune.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     loss_vals\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mitem())\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/fine_tune.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mempty_cache()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/fine_tune.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloss: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39msum\u001b[39m(loss_vals)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(loss_vals)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/flamingo/lib/python3.10/site-packages/torch/cuda/memory.py:133\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Releases all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 133\u001b[0m     torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_emptyCache()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "loss_vals = []\n",
    "for epoch in range(1):\n",
    "    counter = 0\n",
    "    for batch in tqdm.tqdm(pseudo_outputs.values()):\n",
    "\n",
    "        batch = batch[len(instruction)-2:] +\"<|endofchunk|>\"\n",
    "\n",
    "        token = model.tokenizer(batch)\n",
    "        image = data.palceholder_image\n",
    "        image_token = data.image_preprocess_batch(model.image_processor, [image])\n",
    "        #image_tokens = torch.cat([image_token], dim=0)\n",
    "        input_ids, attention_mask = model._prepare_text(batch)\n",
    "\n",
    "        output = model.model(\n",
    "                    image_token.to(0, dtype=torch.bfloat16),\n",
    "                    input_ids[:, :-1],\n",
    "                    attention_mask[:, :-1]\n",
    "                )\n",
    "        loss = criterion(output.logits.reshape(1, 50280, -1), input_ids[:, 1:])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_vals.append(loss.cpu().detach().cpu().item())\n",
    "        torch.cuda.empty_cache()\n",
    "    print(f\"loss: {sum(loss_vals)/len(loss_vals)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2000 [00:00<19:18,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "方法εί\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/2000 [00:01<20:45,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "方法εί\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/2000 [00:03<52:27,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "方法存界存界存εί方法并存εί方法\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/2000 [00:04<37:36,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "方法εί\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/2000 [00:04<27:40,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "方法\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/2000 [00:05<23:14,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "εί\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/2000 [00:05<23:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "方法存εί\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/2000 [00:06<27:13,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "方法存εί\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/2000 [00:07<22:55,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "方法εί\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/2000 [00:08<26:13,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "方法存εί\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/2000 [00:09<25:17,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "方法存εί\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/2000 [00:09<22:02,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "方法εί\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 13/2000 [00:11<40:33,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "方法存εί方法并存εί方法并存εί\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 14/2000 [00:12<33:00,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "方法εί\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 15/2000 [00:13<32:13,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "方法存εί\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 16/2000 [00:14<29:26,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "方法存εί\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 17/2000 [00:14<23:27,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "方法\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 18/2000 [00:15<22:59,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "方法εί\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 19/2000 [00:15<20:36,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "方法εί\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 20/2000 [00:15<17:03,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "方法\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 21/2000 [00:16<18:53,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "方法存εί\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 22/2000 [00:16<17:05,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "方法\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 23/2000 [00:17<17:29,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "方法εί\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 24/2000 [00:17<17:49,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "法εί\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 24/2000 [00:18<25:24,  1.30it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/fine_tune.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/fine_tune.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m data\u001b[39m.\u001b[39;49minfer(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/fine_tune.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     model\u001b[39m.\u001b[39;49mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/fine_tune.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     model\u001b[39m.\u001b[39;49mimage_processor,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/fine_tune.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     model\u001b[39m.\u001b[39;49mtokenizer,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/fine_tune.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39m/mnt/d/datasets/\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/fine_tune.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     early_stop\u001b[39m=\u001b[39;49m\u001b[39m2000\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/tovitu/codes/Instruction-tuned-Flamingo-MLLM/fine_tune.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "File \u001b[0;32m~/codes/Instruction-tuned-Flamingo-MLLM/scripts/datasets.py:390\u001b[0m, in \u001b[0;36mSQUAD_dataset.infer\u001b[0;34m(self, model, image_encoder, text_encoder, output_dir, shots, early_stop)\u001b[0m\n\u001b[1;32m    387\u001b[0m     answer_len \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mMAX_LEN\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    389\u001b[0m \u001b[39m# Inference\u001b[39;00m\n\u001b[0;32m--> 390\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    391\u001b[0m     image_token\u001b[39m.\u001b[39;49mto(device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice, dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision),\n\u001b[1;32m    392\u001b[0m     text_token[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice),\n\u001b[1;32m    393\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mtext_token[\u001b[39m\"\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mto(\n\u001b[1;32m    394\u001b[0m         device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice, dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision\n\u001b[1;32m    395\u001b[0m     ),\n\u001b[1;32m    396\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49manswer_len,\n\u001b[1;32m    397\u001b[0m     num_beams\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m    398\u001b[0m     pad_token_id\u001b[39m=\u001b[39;49m\u001b[39m50277\u001b[39;49m,\n\u001b[1;32m    399\u001b[0m )\n\u001b[1;32m    400\u001b[0m \u001b[39mprint\u001b[39m(answer_len)\n\u001b[1;32m    401\u001b[0m output \u001b[39m=\u001b[39m text_encoder\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/codes/Instruction-tuned-Flamingo-MLLM/open_flamingo/src/flamingo.py:165\u001b[0m, in \u001b[0;36mFlamingo.generate\u001b[0;34m(self, vision_x, lang_x, attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_vision_x(vision_x\u001b[39m=\u001b[39mvision_x)\n\u001b[1;32m    164\u001b[0m eos_token_id \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39meos_token_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meoc_token_id)\n\u001b[0;32m--> 165\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlang_encoder\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    166\u001b[0m     input_ids\u001b[39m=\u001b[39;49mlang_x,\n\u001b[1;32m    167\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    168\u001b[0m     eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m    169\u001b[0m     num_beams\u001b[39m=\u001b[39;49mnum_beams,\n\u001b[1;32m    170\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    171\u001b[0m )\n\u001b[1;32m    173\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlang_encoder\u001b[39m.\u001b[39mclear_conditioned_layers()\n\u001b[1;32m    174\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlang_encoder\u001b[39m.\u001b[39m_use_cached_vision_x \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/flamingo/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/flamingo/lib/python3.10/site-packages/transformers/generation/utils.py:1752\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1746\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1747\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1748\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1749\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1750\u001b[0m     )\n\u001b[1;32m   1751\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1752\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   1753\u001b[0m         input_ids,\n\u001b[1;32m   1754\u001b[0m         beam_scorer,\n\u001b[1;32m   1755\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1756\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1757\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1758\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1759\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1760\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1761\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1762\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1763\u001b[0m     )\n\u001b[1;32m   1765\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE:\n\u001b[1;32m   1766\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/miniconda3/envs/flamingo/lib/python3.10/site-packages/transformers/generation/utils.py:3144\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3141\u001b[0m next_tokens \u001b[39m=\u001b[39m next_tokens \u001b[39m%\u001b[39m vocab_size\n\u001b[1;32m   3143\u001b[0m \u001b[39m# stateless\u001b[39;00m\n\u001b[0;32m-> 3144\u001b[0m beam_outputs \u001b[39m=\u001b[39m beam_scorer\u001b[39m.\u001b[39;49mprocess(\n\u001b[1;32m   3145\u001b[0m     input_ids,\n\u001b[1;32m   3146\u001b[0m     next_token_scores,\n\u001b[1;32m   3147\u001b[0m     next_tokens,\n\u001b[1;32m   3148\u001b[0m     next_indices,\n\u001b[1;32m   3149\u001b[0m     pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   3150\u001b[0m     eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   3151\u001b[0m     beam_indices\u001b[39m=\u001b[39;49mbeam_indices,\n\u001b[1;32m   3152\u001b[0m )\n\u001b[1;32m   3154\u001b[0m beam_scores \u001b[39m=\u001b[39m beam_outputs[\u001b[39m\"\u001b[39m\u001b[39mnext_beam_scores\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   3155\u001b[0m beam_next_tokens \u001b[39m=\u001b[39m beam_outputs[\u001b[39m\"\u001b[39m\u001b[39mnext_beam_tokens\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/flamingo/lib/python3.10/site-packages/transformers/generation/beam_search.py:251\u001b[0m, in \u001b[0;36mBeamSearchScorer.process\u001b[0;34m(self, input_ids, next_scores, next_tokens, next_indices, pad_token_id, eos_token_id, beam_indices, group_index)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_size):\n\u001b[1;32m    250\u001b[0m     batch_group_idx \u001b[39m=\u001b[39m batch_idx \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_beam_groups \u001b[39m+\u001b[39m group_index\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_done[batch_group_idx]:\n\u001b[1;32m    252\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_beams \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_beam_hyps[batch_group_idx]):\n\u001b[1;32m    253\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatch can only be done if at least \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_beams\u001b[39m}\u001b[39;00m\u001b[39m beams have been generated\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data.infer(\n",
    "    model.model,\n",
    "    model.image_processor,\n",
    "    model.tokenizer,\n",
    "    '/mnt/d/datasets/',\n",
    "    early_stop=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 0.0, 'f1': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from evaluate import load\n",
    "\n",
    "DATASET_DIR = \"/mnt/d/datasets/\"\n",
    "\n",
    "with open(f\"{DATASET_DIR}squad_resutls.json\", \"r\") as f:\n",
    "    predictions = json.load(f)\n",
    "with open(f\"{DATASET_DIR}squad_references.json\", \"r\") as f:\n",
    "    references = json.load(f)\n",
    "\n",
    "squad_metric = load(\"squad\")\n",
    "results = squad_metric.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = lora_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(merged.state_dict(), '/mnt/d/models/fine-tuned-nl-flamingo/checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello?<|endofchunk|> Hi<|endofchunk|>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello?<|endofchunk|> Hi\"\n",
    "image = data.palceholder_image\n",
    "image_token = data.image_preprocess_batch(model.image_processor, [image])\n",
    "\n",
    "input_ids, attention_mask = model._prepare_text(text)\n",
    "output = model.model.generate(\n",
    "    image_token.to(0, dtype=torch.bfloat16),\n",
    "    input_ids,\n",
    "    attention_mask,\n",
    "    max_length=1000,\n",
    "    num_beams=3,\n",
    "    pad_token_id=50277,\n",
    ")\n",
    "answer = model.tokenizer.decode(output[0])\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flamingo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
