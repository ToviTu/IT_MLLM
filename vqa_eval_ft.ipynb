{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.model import EvalModel\n",
    "import os\n",
    "from scripts.datasets import SQUAD_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from scripts.download_dataset import DATASET_DIR\n",
    "from scripts.custom_evaluate import evaluate_vqa\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "from scripts.download_dataset import DATASET_DIR\n",
    "from scripts.download_dataset import DATASET_URL\n",
    "from open_flamingo.eval.eval_datasets import VQADataset\n",
    "from open_flamingo.eval.vqa_metric import (\n",
    "    compute_vqa_accuracy,\n",
    "    postprocess_vqa_generation,\n",
    ")\n",
    "from open_flamingo.eval.eval_model import BaseEvalModel\n",
    "import scripts.utils as utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tovitu/.cache/huggingface/modules/transformers_modules/anas-awadalla/mpt-1b-redpajama-200b/50d6bc94e17812873f39c36c5f815263fa71fb80/attention.py:289: UserWarning: Using `attn_impl: torch`. If your model does not use `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n",
      "Flamingo model initialized with 1046992944 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_DIR = (\n",
    "    \"/mnt/d/models/\"\n",
    "    if os.environ.get(\"CHECKPOINT_DIR\") == None\n",
    "    else os.environ[\"CHECKPOINT_DIR\"]\n",
    ")\n",
    "\n",
    "model_args = {\n",
    "    \"vision_encoder_path\": \"ViT-L-14\",\n",
    "    \"vision_encoder_pretrained\": \"openai\",\n",
    "    \"lm_path\": \"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "    \"lm_tokenizer_path\": \"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "    \"checkpoint_path\": f\"{CHECKPOINT_DIR}fine-tuned-nl-flamingo-visual/visual_checkpoint.pt\",\n",
    "    #\"checkpoint_path\": \"/mnt/d/models/OpenFlamingo-3B-vitl-mpt1b/checkpoint.pt\",\n",
    "    \"cross_attn_every_n_layers\": 1,\n",
    "    \"precision\": \"bf16\",\n",
    "    \"device\": 0,\n",
    "}\n",
    "\n",
    "model = EvalModel(\n",
    "    model_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/mnt/d/datasets/v2_OpenEnded_mscoco_val2014_questions.json\", 'r') as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "with open(\"/mnt/d/datasets/v2_mscoco_val2014_annotations.json\", 'r') as f:\n",
    "    annos = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = '/mnt/d/datasets/'\n",
    "dataset_name = 'vqav2'\n",
    "\n",
    "train_image_dir_path = DATASET_DIR + \"train2014\"\n",
    "train_questions_json_path = (\n",
    "    DATASET_DIR + \"v2_OpenEnded_mscoco_train2014_questions.json\"\n",
    ")\n",
    "train_annotations_json_path = DATASET_DIR + \"v2_mscoco_train2014_annotations.json\"\n",
    "test_image_dir_path = DATASET_DIR + \"val2014\"\n",
    "test_questions_json_path = (\n",
    "    DATASET_DIR + \"v2_OpenEnded_mscoco_val2014_questions.json\"\n",
    ")\n",
    "test_annotations_json_path = DATASET_DIR + \"v2_mscoco_val2014_annotations.json\"\n",
    "\n",
    "train_dataset = VQADataset(\n",
    "    image_dir_path=train_image_dir_path,\n",
    "    question_path=train_questions_json_path,\n",
    "    annotations_path=train_annotations_json_path,\n",
    "    is_train=True,\n",
    "    dataset_name=dataset_name,\n",
    ")\n",
    "\n",
    "test_dataset = VQADataset(\n",
    "    image_dir_path=test_image_dir_path,\n",
    "    question_path=test_questions_json_path,\n",
    "    annotations_path=test_annotations_json_path,\n",
    "    is_train=False,\n",
    "    dataset_name=dataset_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=1, collate_fn=utils.custom_collate_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = lambda question: f\"Question: {question}\\nContext: <image>\\nRationale:\"\n",
    "context_template = lambda question, answer: f\"Question: {question}\\nContext: <image>\\nRationale: Answer {answer}<|endofchunk|>\"\n",
    "query_set = utils.get_query_set(train_dataset, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference vqav2:   0%|          | 0/214354 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference vqav2:   1%|          | 1999/214354 [1:07:42<119:53:09,  2.03s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "acc = []\n",
    "\n",
    "counter = 0\n",
    "for batch in tqdm(\n",
    "        test_dataloader,\n",
    "        desc=f\"Running inference {dataset_name}\",\n",
    "    ):\n",
    "\n",
    "    counter += 1\n",
    "    if counter >= 2000:\n",
    "        break\n",
    "    \n",
    "    # batch_demo_samples = utils.sample_batch_demos_from_query_set(\n",
    "    #         query_set, 4, len(batch[\"image\"])\n",
    "    #     )\n",
    "\n",
    "    batch_images, batch_text = [], []\n",
    "    for i in range(len(batch[\"image\"])):\n",
    "        # if 4 > 0:\n",
    "        #     context_images = [x[\"image\"] for x in batch_demo_samples[i]]\n",
    "        # else:\n",
    "        #     context_images = []\n",
    "        # batch_images.append([batch[\"image\"][i]])\n",
    "\n",
    "        # context_text = \"\".join(\n",
    "        #     [\n",
    "        #         model.get_vqa_prompt(\n",
    "        #             question=x[\"question\"], answer=x[\"answers\"][0]\n",
    "        #         )\n",
    "        #         + \"\\n\"\n",
    "        #         for x in batch_demo_samples[i]\n",
    "        #     ]\n",
    "        # )\n",
    "\n",
    "        batch_images.append([batch[\"image\"][i]])\n",
    "        batch_text.append(\n",
    "            prompt_template(question=batch[\"question\"][i])\n",
    "            )\n",
    "        \n",
    "        outputs = model.get_outputs(\n",
    "            batch_images=batch_images,\n",
    "            batch_text=batch_text,\n",
    "            min_generation_length=0,\n",
    "            max_generation_length=100,\n",
    "            num_beams=3,\n",
    "            length_penalty=0.0,\n",
    "        )\n",
    "\n",
    "        process_function = postprocess_vqa_generation\n",
    "        new_predictions = map(process_function, outputs)\n",
    "\n",
    "        for new_prediction, sample_id in zip(new_predictions, batch[\"question_id\"]):\n",
    "            predictions.append({\"answer\": new_prediction, \"question_id\": sample_id})\n",
    "        \n",
    "        match = 0\n",
    "        for answer in batch['answers'][0]:\n",
    "            if answer.lower() in new_prediction.lower():\n",
    "                match += 1\n",
    "        acc.append(1 if (match / 3) >= 1 else (match / 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer': ' He is looking at a skateboard ramp.\\n\\n',\n",
       "  'question_id': 262148000},\n",
       " {'answer': ' The people in the background are skateboarders.\\n\\n',\n",
       "  'question_id': 262148001},\n",
       " {'answer': ' He is on top of a skateboard.\\n\\n', 'question_id': 262148002},\n",
       " {'answer': ' The picture of a bowl of noodle soup was copyrighted by a website.\\n\\n',\n",
       "  'question_id': 393225000}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2696348174087044"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{dataset_name}results.json\", \"w\") as f:\n",
    "        f.write(json.dumps(predictions, indent=4))\n",
    "\n",
    "acc = -1\n",
    "if test_annotations_json_path is not None:\n",
    "    acc = compute_vqa_accuracy(\n",
    "        f\"{dataset_name}results.json\",\n",
    "        test_questions_json_path,\n",
    "        test_annotations_json_path,\n",
    "    )\n",
    "    # delete the temporary file\n",
    "    os.remove(f\"{dataset_name}results.json\")\n",
    "\n",
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flamingo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
