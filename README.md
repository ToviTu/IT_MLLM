# Instruction-tuned-Flamingo-MLLM

Final project for CSE527A Natural Language Processing at Washington Unversity in Saint Louis. 

## Abstract

Recent advancements in Large Language Mod-001
els have enabled Multimodal large language002
models capable of reasoning with vision and003
audio signals. However, a common obstacle004
in building MLLM systems is that end-to-end005
training causes a significant performance drop006
in natural language tasks. In this work, We007
present a fine-tuning method to enhance the nat-008
ural language understanding abilities by align-009
ing MLLMs to a performant LLM in a knowl-010
edge distillation fashion. Model-generated in-011
context images and responses augment existing012
datasets with bimodal supervision, strengthen-013
ing MLLM’s performance in a task-specific014
0-shot setting. Our method steadily boosts nat-015
ural language performance while maintaining016
visual understanding. In the experiments with a017
small-scaled MLLM, we show improvement in018
both vision-language and language-only tasks,019
yet with small training sessions and limited020
training samples.

![Uploading pipeline.png…]()
